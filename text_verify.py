from fastapi import APIRouter
from transformers import pipeline
import wikipedia

router = APIRouter()

# -----------------------------
# Load AI text detector
# -----------------------------
detector = pipeline(
    "text-classification",
    model="roberta-base-openai-detector"
)

# -----------------------------
# Relation-based fact rules
# -----------------------------
def relation_validation(text: str):
    t = text.lower()

    # Geography rules
    if "karnataka" in t:
        if "europe" in t:
            return {
                "accuracy_score": 5,
                "accuracy_verdict": "Factually Incorrect",
                "evidence": "Karnataka is a state in India (Asia), not Europe."
            }
        if "india" in t or "asia" in t:
            return {
                "accuracy_score": 95,
                "accuracy_verdict": "Factually Correct",
                "evidence": "Karnataka is a state located in India, which is part of Asia."
            }

    if "sun rises in the west" in t:
        return {
            "accuracy_score": 5,
            "accuracy_verdict": "Factually Incorrect",
            "evidence": "The Sun rises in the east due to Earth's rotation."
        }

    if "sun rises in the east" in t:
        return {
            "accuracy_score": 95,
            "accuracy_verdict": "Factually Correct",
            "evidence": "The Sun appears to rise in the east due to Earth's rotation."
        }

    return None

# -----------------------------
# Wikipedia-based verification
# -----------------------------
def wikipedia_verification(text: str):
    try:
        summary = wikipedia.summary(text, sentences=2)
        return {
            "accuracy_score": 85,
            "accuracy_verdict": "Factually Correct",
            "evidence": summary
        }
    except wikipedia.exceptions.DisambiguationError:
        return {
            "accuracy_score": 60,
            "accuracy_verdict": "Partially Verifiable",
            "evidence": "Multiple interpretations found."
        }
    except wikipedia.exceptions.PageError:
        return {
            "accuracy_score": 30,
            "accuracy_verdict": "No Reliable Source Found",
            "evidence": "No matching encyclopedic source available."
        }
    except Exception:
        return {
            "accuracy_score": 40,
            "accuracy_verdict": "Uncertain",
            "evidence": "Unable to verify the content."
        }

# ==================================================
# âœ… NEW: REUSABLE TEXT VERIFICATION LOGIC (ADDED)
# ==================================================
def verify_text_logic(text: str):
    # -------- AI Authorship Detection --------
    result = detector(text)[0]
    ai_prob = int(result["score"] * 100)

    if ai_prob >= 80:
        authorship = "Likely AI-Generated"
    elif ai_prob >= 50:
        authorship = "Possibly AI-Generated"
    else:
        authorship = "Likely Human-Written"

    # -------- Factual Verification --------
    rule_result = relation_validation(text)

    if rule_result:
        accuracy_result = rule_result
    else:
        accuracy_result = wikipedia_verification(text)

    return {
        "ai_generated_probability": ai_prob,
        "authorship": authorship,
        "accuracy_score": accuracy_result["accuracy_score"],
        "accuracy_verdict": accuracy_result["accuracy_verdict"],
        "evidence": accuracy_result["evidence"]
    }

# -----------------------------
# TEXT VERIFICATION API
# -----------------------------
@router.post("/verify")
async def verify_text(data: dict):
    text = data.get("text", "").strip()

    if not text:
        return {
            "status": "error",
            "message": "No text provided"
        }

    result = verify_text_logic(text)

    return {
        "status": "success",
        **result
    }
